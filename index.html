<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>PrettyBow by wangxiangtao</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
   <div id="nav">
        <p class="remove-bottom">PrettyBow  <img src="https://raw.githubusercontent.com/wangxiangtao/PrettyMatch/master/dragonfly.png" style="height: 1.9em; vertical-align: -0.2em">
       
       </p>
    </div>
    <div class="shadow"> </div>
    <div class="wrapper">
      <header>
        <h1>PrettyBow <img src="https://raw.githubusercontent.com/wangxiangtao/PrettyMatch/master/dragonfly.png" style="height: 1em; vertical-align: -0.1em"></h1>
        <p>A Scala Library for Statistical Language Modeling, Text Retrieval and Classification</p>

        <ul>
          <li><a href="https://github.com/wangxiangtao/PrettyBow/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/wangxiangtao/PrettyBow/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/wangxiangtao/PrettyBow">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
     
<p>PrettyBow is a library of Scala for Statistical Language Modeling, Text Retrieval and Classification.  It is built on the top 
of Lucene and Weka. 
The idea of PrettyBow is inspired by <a href="http://www.cs.cmu.edu/~mccallum/bow/">Bow (a library written in C)</a> and <a href='http://www.cs.cmu.edu/~cprose/TagHelper.html'>TagHelper (a library written in Java)</a></p>

<p>Recursively descending directories, finding text files.
Finding `document' boundaries when there are multiple documents per file.
Tokenizing a text file, according to several different methods.
Including N-grams among the tokens.
Mapping strings to integers and back again, very efficiently.
Building a sparse matrix of document/token counts.
Pruning vocabulary by word counts or by information gain.
Building and manipulating word vectors.
Setting word vector weights according to Naive Bayes, TFIDF, and several other methods.
Smoothing word probabilities according to Laplace (Dirichlet uniform), M-estimates, Witten-Bell, and Good-Turning.
Scoring queries for retrieval or classification.
Writing all data structures to disk in a compact format.
Reading the document/token matrix from disk in an efficient, sparse fashion.
Performing test/train splits, and automatic classification tests.
Operating in server mode, receiving and answering queries over a socket.</p>

<p>Recursively descending directories, finding text files.
Finding `document' boundaries when there are multiple documents per file.
Tokenizing a text file, according to several different methods.
Including N-grams among the tokens.
Mapping strings to integers and back again, very efficiently.
Building a sparse matrix of document/token counts.
Pruning vocabulary by word counts or by information gain.
Building and manipulating word vectors.
Setting word vector weights according to Naive Bayes, TFIDF, and several other methods.
Smoothing word probabilities according to Laplace (Dirichlet uniform), M-estimates, Witten-Bell, and Good-Turning.
Scoring queries for retrieval or classification.
Writing all data structures to disk in a compact format.
Reading the document/token matrix from disk in an efficient, sparse fashion.
Performing test/train splits, and automatic classification tests.
Operating in server mode, receiving and answering queries over a socket.</p>

<p>Recursively descending directories, finding text files.
Finding `document' boundaries when there are multiple documents per file.
Tokenizing a text file, according to several different methods.
Including N-grams among the tokens.
Mapping strings to integers and back again, very efficiently.
Building a sparse matrix of document/token counts.
Pruning vocabulary by word counts or by information gain.
Building and manipulating word vectors.
Setting word vector weights according to Naive Bayes, TFIDF, and several other methods.
Smoothing word probabilities according to Laplace (Dirichlet uniform), M-estimates, Witten-Bell, and Good-Turning.
Scoring queries for retrieval or classification.
Writing all data structures to disk in a compact format.
Reading the document/token matrix from disk in an efficient, sparse fashion.
Performing test/train splits, and automatic classification tests.
Operating in server mode, receiving and answering queries over a socket.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/wangxiangtao">wangxiangtao</a></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
